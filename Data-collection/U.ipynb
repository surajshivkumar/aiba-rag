{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../pages/dezhi-yin.htm',\n",
       " '../pages/GA.htm',\n",
       " '../pages/he-zhang.htm',\n",
       " '../pages/Locations.htm',\n",
       " '../pages/Application-Process.htm',\n",
       " '../pages/ManishAgarwal.htm',\n",
       " '../pages/tim-smith.htm',\n",
       " '../pages/Wolfgang.htm',\n",
       " '../pages/Alan-Henver.htm',\n",
       " '../pages/immunization.htm',\n",
       " '../pages/Shivendu.htm',\n",
       " '../pages/GBAIS.htm',\n",
       " '../pages/Ron-satterfield.htm',\n",
       " '../pages/Joni-jones.htm',\n",
       " '../pages/DonBerndt.htm',\n",
       " '../pages/tom-stablein.htm',\n",
       " '../pages/Mathew.htm',\n",
       " '../pages/Homepage.htm',\n",
       " '../pages/bull-runner-hours.htm',\n",
       " '../pages/Grandon-Gill.htm',\n",
       " '../pages/barb-W.htm',\n",
       " '../pages/FAQ.htm',\n",
       " '../pages/Anol.htm',\n",
       " '../pages/Insurance.htm',\n",
       " '../pages/bull-runner.htm',\n",
       " '../pages/Sunil-Mithas.htm',\n",
       " '../pages/Daniel-Z.htm',\n",
       " '../pages/Faculty.htm']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_dir = '../pages/'\n",
    "files = os.listdir(base_dir)\n",
    "files = [base_dir + file for file in files if '.htm' in file]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = files[-1]\n",
    "with open(file1, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Manish Agrawal\\xa0• Professor • TampaDon Berndt\\xa0• Associate Professor • TampaAnol Bhattacherjee\\xa0 • Professor • TampaKaushik Dutta\\xa0• Information Systems and Management Director and ProfessorGrandon Gill\\xa0• Professor and Academic Director, DBA programAlan R. Hevner\\xa0• Distinguished University Professor and AAAS FellowWolfgang S. Jank, Anderson Professor and advisor, Center for Analytics and Creativity Joni L. Jones\\xa0• Associate Professor • Academic Director, MS in BAIS program • Faculty Liaison,\\n               Florida Center for Cybersecurity and Academic Director • TampaSunil Mithas\\xa0• Professor and World Class Scholar • TampaMatthew Mullarkey\\xa0• Associate Professor of Instruction and Director, DBA programRonald K. Satterfield\\xa0• Professor of Instruction • Academic Director, Weekend Executive MS program• Instructor\\n               • Member, Board of Directors, USF Federal Credit Union • TampaShivendu Shivendu\\xa0• Associate Professor • TampaTim Smith • Professor of Instruction • St. PetersburgTom Stablein\\xa0• Assistant Professor of Instruction • Director, Weekend Executive MS program Dezhi Yin\\xa0• Assistant Professor • TampaDaniel Zantedeschi\\xa0• Assistant Professor • TampaHe Zhang\\xa0• Assistant Professor • TampaBarbara Warner\\xa0• Senior Instructor • Advisor, MS in BAIS program'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(content, 'html.parser')\n",
    "title = soup.title.string if soup.title else \"No title\"\n",
    "target_div = soup.find('div', class_='mainContent_well u-flexItem--largeExtra')\n",
    "paragraphs = target_div.find_all('p')\n",
    "main_content = ' '.join(p.get_text().strip() for p in paragraphs)\n",
    "    \n",
    "metadata = {\n",
    "    'keywords': soup.find('meta', attrs={'name': 'keywords'})['content'] if soup.find('meta', attrs={'name': 'keywords'}) else \"\",\n",
    "    'description': soup.find('meta', attrs={'name': 'description'})['content'] if soup.find('meta', attrs={'name': 'description'}) else \"\",\n",
    "    # Add more metadata extraction as needed\n",
    "}\n",
    "main_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 28 files. JSON files saved in ../pages/json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "class HTMLExtractor:\n",
    "    def __init__(self):\n",
    "        self.extractors = {}\n",
    "\n",
    "    def add_extractor(self, name, title_func, content_func, metadata_func):\n",
    "        self.extractors[name] = {\n",
    "            'title': title_func,\n",
    "            'content': content_func,\n",
    "            'metadata': metadata_func\n",
    "        }\n",
    "\n",
    "    def extract(self, html, extractor_name):\n",
    "        extractor = self.extractors.get(extractor_name)\n",
    "        if not extractor:\n",
    "            return None\n",
    "        \n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        return {\n",
    "            'title': extractor['title'](soup),\n",
    "            'content': extractor['content'](soup),\n",
    "            'metadata': extractor['metadata'](soup)\n",
    "        }\n",
    "\n",
    "    def guess_extractor(self, html):\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        for name, extractor in self.extractors.items():\n",
    "            if extractor['content'](soup):\n",
    "                return name\n",
    "        return None\n",
    "\n",
    "def process_htm_file(file_path, extractor, output_dir):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    extractor_name = extractor.guess_extractor(content)\n",
    "    if extractor_name:\n",
    "        extracted_data = extractor.extract(content, extractor_name)\n",
    "        if extracted_data:\n",
    "            extracted_data['extractor_used'] = extractor_name\n",
    "            extracted_data['source'] = file_path\n",
    "\n",
    "            # Create a LangChain Document\n",
    "            doc = Document(\n",
    "                page_content=extracted_data['content'],\n",
    "                metadata={\n",
    "                    'source': file_path,\n",
    "                    'title': extracted_data['title'],\n",
    "                    **extracted_data['metadata'],\n",
    "                    'extractor_used': extractor_name\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Use RecursiveCharacterTextSplitter\n",
    "            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "            splits = text_splitter.split_documents([doc])\n",
    "\n",
    "            # Prepare data for JSON\n",
    "            doc_data = {\n",
    "                'metadata': doc.metadata,\n",
    "                'chunks': [\n",
    "                    {\n",
    "                        'content': split.page_content,\n",
    "                        'metadata': split.metadata\n",
    "                    } for split in splits\n",
    "                ]\n",
    "            }\n",
    "\n",
    "            # Save to individual JSON file\n",
    "            output_file = os.path.join(output_dir, f\"{os.path.basename(file_path)}.json\")\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(doc_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "            return output_file\n",
    "    \n",
    "    print(f\"Failed to extract data from {file_path}\")\n",
    "    return None\n",
    "\n",
    "def process_directory(input_dir, output_dir, extractor):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    processed_files = []\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith('.htm') or filename.endswith('.html'):\n",
    "            file_path = os.path.join(input_dir, filename)\n",
    "            output_file = process_htm_file(file_path, extractor, output_dir)\n",
    "            if output_file:\n",
    "                processed_files.append(output_file)\n",
    "    \n",
    "    return processed_files\n",
    "\n",
    "# Example usage\n",
    "extractor = HTMLExtractor()\n",
    "\n",
    "# Add extractor for pages with a specific div class\n",
    "extractor.add_extractor(\n",
    "    'main_content_well',\n",
    "    lambda soup: soup.title.string if soup.title else \"No title\",\n",
    "    lambda soup: ' '.join(p.get_text().strip() for p in soup.find('div', class_='mainContent_well u-flexItem--largeExtra').find_all('p')) if soup.find('div', class_='mainContent_well u-flexItem--largeExtra') else \"\",\n",
    "    lambda soup: {\n",
    "        'keywords': soup.find('meta', attrs={'name': 'keywords'})['content'] if soup.find('meta', attrs={'name': 'keywords'}) else \"\",\n",
    "        'description': soup.find('meta', attrs={'name': 'description'})['content'] if soup.find('meta', attrs={'name': 'description'}) else \"\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Add extractor for pages with a specific article structure\n",
    "extractor.add_extractor(\n",
    "    'article_content',\n",
    "    lambda soup: soup.find('h1', class_='article-title').get_text() if soup.find('h1', class_='article-title') else \"No title\",\n",
    "    lambda soup: ' '.join(p.get_text().strip() for p in soup.find('article', class_='main-content').find_all('p')) if soup.find('article', class_='main-content') else \"\",\n",
    "    lambda soup: {\n",
    "        'author': soup.find('span', class_='author-name').get_text() if soup.find('span', class_='author-name') else \"Unknown\",\n",
    "        'date': soup.find('time', class_='publish-date')['datetime'] if soup.find('time', class_='publish-date') else \"Unknown\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Add extractors as before\n",
    "# ... [Add your extractors here]\n",
    "\n",
    "# Process the directory\n",
    "input_directory = '../pages'\n",
    "output_directory = '../pages/json'\n",
    "processed_files = process_directory(input_directory, output_directory, extractor)\n",
    "\n",
    "print(f\"Processed {len(processed_files)} files. JSON files saved in {output_directory}\")\n",
    "\n",
    "# Example us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
